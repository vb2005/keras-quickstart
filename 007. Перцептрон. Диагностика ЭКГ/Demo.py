# Для работы потребуются: Бибилотеки для работы с данными (Pandas, Numpy)
import numpy as np
import pandas as pd

# Библиотека Keras. Последовательная модель и 2 вида слоёв: полносвязный и активационный
from keras import Sequential
from keras.layers import *

# Построитель графиков. Для наглядности
from matplotlib import pyplot as plt

# Читаем данные из CSV и делаем из них массив Numpy.
# Попробуем сделать это через Pandas
df = pd.read_csv('Arrhythmia.csv', sep=";",header=None)
data = df.to_numpy()

# Последний столбец - диагноз. 1 - если здоров, 2..18 - коды заболеваний.
# Нейронная сеть будет лишь выявлять: здоров пациент или не очень.
# Поэтому 1 меняем на 0, а всё остальное на 1.

for i in range(452):
  if data[i,276] <= 1:
    data[i,276] = 0
  else:
    data[i,276] = 1
    
# Разделение данных на вход и выход
X = data[:,:276]
Y = data[:,276]

# Поле для творчества. Слои и функции активации на Ваш выбор
# каждая новая строка - новый скрытый слой внутри сети
# В качестве аргумента указывается активационная функция для всех нейронов данного слоя и их количество
# Попробуйте самостоятельно описать архитектуру, которая даст лучшие показатели,
# чем эта архитектура. Сравните результаты в группе

model = Sequential()

# Первый слой - 274 нейрона (по размеру входного вектора). Далее данные приводятся при помощи функции активации Sigmoid
model.add(Input(276))

# Второй слой и последующие задают глубину обучения и количество весовых коэфициентов.
# Далеко не всегда разумно бесконечно наращивать глубину сети. Итогда сеть и из 2х слоёв
# Справляется с указанной задачей
model.add(Dense(200,"sigmoid"))
model.add(Dense(200,"sigmoid"))

# Выходной слой с 1 значением. У нас это индикатор наличия или отсутсвия заболевания
model.add(Dense(1,"sigmoid"))

# Выбираем оптимизатор и функцию потерь и запускаем обучение.
# Для того, чтобы оценить точность работы на разных архитектурах
# Используем во всех экспериментах равное число эпох и одинаковые функции потерь и активации

model.compile("Adam","MSE", metrics = ["accuracy"])            # Сборка графа. Указание оптимизатора и функции потерь
history = model.fit(                   # Результат обучения сохраняется в переменную history
    X,Y,                               # Обучающая выборка
    batch_size=32,                     # Размер посылки
    epochs=50,                         # Число эпох. Используем 50.
    validation_split=0.2)              # Выборка для валидации

# Визуализация графика
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Функция потерь')
plt.ylabel('Потери (меньше - лучше)')
plt.xlabel('Число эпох')
plt.legend(['Обучающая', 'Тестовая'], loc='upper left')
plt.show()

# Получаем предсказнанные на основании нейронной сети значения
# Здесь для оценки применяется исходный массив X, что неправильно (объясните, почему?)
# Вычисляется разница между предсказанным значением и истинным.
pred_Y = model.predict(X).ravel()

# Вычисляем сумму модулей поэлементной разницы двух массивови делим на количество примеров
difference = pow((pred_Y - Y),2).sum() / len(pred_Y)

# Получаем итоговую точность работы на указанных примерах. Больше - лучше
print('Точность работы:', (100-difference*100), '%')